# LLM Daily Brief — 2026-01-30

- 生成时间：2026-01-30 21:03 (Asia/Shanghai)
- 抓取窗口：24h · 源数：14


## 中文媒体

- [基于数万次真机评测，RoboChallenge首份年度报告发布](https://www.qbitai.com/2026/01/374597.html) — 量子位 · 2026-01-30 20:18
- [这个真人版《火影忍者》竟然是AI做的，来自中国AI视频新王者Vidu Q3](https://www.qbitai.com/2026/01/374563.html) — 量子位 · 2026-01-30 19:39
- [顶尖模型离“科学家”还差得远？AI4S亟待迈向2.0时代](https://www.jiqizhixin.com/articles/2026-01-30-11) — 机器之心 · 2026-01-30 18:57
- [大模型的第一性原理：（二）信号处理篇](https://www.jiqizhixin.com/articles/2026-01-30-10) — 机器之心 · 2026-01-30 18:21
- [别只练力量和心肺，人生下半场拼的是肌肉爆发力](https://sspai.com/prime/story/a-guide-to-muscle-explosive-power) — 少数派 · 2026-01-30 18:16
- [谷歌开放世界模型一夜刷屏，AI游戏门槛归零时刻来了？](https://www.jiqizhixin.com/articles/2026-01-30-9) — 机器之心 · 2026-01-30 18:06
- [本周看什么 | 最近值得一看的 8 部作品](https://sspai.com/post/105907) — 少数派 · 2026-01-30 18:04
- [揭秘！RLVR/GRPO中那些长期被忽略的关键缺陷](https://www.jiqizhixin.com/articles/2026-01-30-8) — 机器之心 · 2026-01-30 17:37
- [商汤开源 SenseNova-MARS，突破多模态搜索推理天花板](https://www.qbitai.com/2026/01/374540.html) — 量子位 · 2026-01-30 17:33
- [千问C端应用团队一口气四篇论文入选ICLR 2026国际顶会！](https://www.qbitai.com/2026/01/374507.html) — 量子位 · 2026-01-30 16:53
- [Waymo遭遇鬼探头，Robotaxi在校门口把儿童给撞了](https://www.qbitai.com/2026/01/374506.html) — 量子位 · 2026-01-30 16:52
- [千问C端应用团队一口气四篇论文入选ICLR 2026国际顶会！](https://www.jiqizhixin.com/articles/2026-01-30-7) — 机器之心 · 2026-01-30 16:29
- [停更但好用的 DriveDroid：把 Android 手机变成 USB 启动盘](https://sspai.com/post/104564) — 少数派 · 2026-01-30 15:00
- [你真的是记性不好吗？关于记忆和遗忘的常见误区](https://sspai.com/post/105677) — 少数派 · 2026-01-30 11:13


## 英文媒体 / Newsletter

- [[AINews] SpaceXai Grok Imagine API - the #1 Video Model, Best Pricing and Latency](https://www.latent.space/p/ainews-spacexai-grok-imagine-api) — Latent Space · 2026-01-30 14:25


## 论文 · arXiv

- [RedSage: A Cybersecurity Generalist LLM](https://arxiv.org/abs/2601.22159v1) — arXiv · LLM core · 2026-01-30 02:59
- [UEval: A Benchmark for Unified Multimodal Generation](https://arxiv.org/abs/2601.22155v1) — arXiv · LLM core · 2026-01-30 02:59
- [DynaWeb: Model-Based Reinforcement Learning of Web Agents](https://arxiv.org/abs/2601.22149v1) — arXiv · LLM core · 2026-01-30 02:59
- [Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts](https://arxiv.org/abs/2601.22156v1) — arXiv · MoE & Reasoning · 2026-01-30 02:59
- [Exploring Reasoning Reward Model for Agents](https://arxiv.org/abs/2601.22154v1) — arXiv · MoE & Reasoning · 2026-01-30 02:59
- [FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale](https://arxiv.org/abs/2601.22146v1) — arXiv · LLM core · 2026-01-30 02:58
- [Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers](https://arxiv.org/abs/2601.22139v1) — arXiv · LLM core · 2026-01-30 02:56
- [StepShield: When, Not Whether to Intervene on Rogue Agents](https://arxiv.org/abs/2601.22136v1) — arXiv · LLM core · 2026-01-30 02:55
- [Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference](https://arxiv.org/abs/2601.22132v1) — arXiv · LLM core · 2026-01-30 02:52
- [World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems](https://arxiv.org/abs/2601.22130v1) — arXiv · LLM core · 2026-01-30 02:51
- [SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents](https://arxiv.org/abs/2601.22129v1) — arXiv · LLM core · 2026-01-30 02:50
- [The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR](https://arxiv.org/abs/2601.22128v1) — arXiv · LLM core · 2026-01-30 02:49
- [Value-Based Pre-Training with Downstream Feedback](https://arxiv.org/abs/2601.22108v1) — arXiv · MoE & Reasoning · 2026-01-30 02:38
- [ECO: Quantized Training without Full-Precision Master Weights](https://arxiv.org/abs/2601.22101v1) — arXiv · MoE & Reasoning · 2026-01-30 02:35
- [Where Do the Joules Go? Diagnosing Inference Energy Consumption](https://arxiv.org/abs/2601.22076v1) — arXiv · MoE & Reasoning · 2026-01-30 02:16

---
数据源与分组在 config.yaml → sections / feeds 中可自定义。