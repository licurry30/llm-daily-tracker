# LLM Daily Brief — 2025-12-19

- 生成时间：2025-12-19 20:49 (Asia/Shanghai)
- 抓取窗口：24h · 源数：14


## 官方 / 厂商 · 国外

- [Elevated error rates on Sonnet 4.5](https://status.claude.com/incidents/8d87293jq0mk) — Anthropic · Status · 2025-12-19 15:48
- [Increased error rates for Sora API](https://status.openai.com//incidents/01KCTM1YZ1AW72M9JVW48TNJES) — OpenAI · Status · 2025-12-19 14:58


## 中文媒体

- [菜鸟与九识智能“超级整合”在即，无人货运“马太效应”凸显](https://www.qbitai.com/2025/12/362185.html) — 量子位 · 2025-12-19 18:45
- [本周看什么 | 最近值得一看的 10 部作品](https://sspai.com/post/104747) — 少数派 · 2025-12-19 18:10
- [破局工业自动化界哥德巴赫猜想，它石智航以全栈技术掀开高价值场景序幕](https://www.jiqizhixin.com/articles/2025-12-19-11) — 机器之心 · 2025-12-19 16:53
- [当年带你上网冲浪的头号老玩家，这回是真AI上头了](https://www.qbitai.com/2025/12/362082.html) — 量子位 · 2025-12-19 15:26
- [用创意守护飞鸟，让年味更有温度：防鸟撞新春窗花设计公益征集活动](https://sspai.com/post/104702) — 少数派 · 2025-12-19 15:11
- [拆解CANN：当华为决定打开算力的「黑盒」](https://www.jiqizhixin.com/articles/2025-12-19-10) — 机器之心 · 2025-12-19 15:09
- [让“组织AI”追上“物理AI”，飞书广州峰会发布“粤企一齐飞”加速计划](https://www.qbitai.com/2025/12/362097.html) — 量子位 · 2025-12-19 15:08
- [Mamba作者团队提出SonicMoE：一个Token舍入，让MoE训练速度提升近2倍](https://www.jiqizhixin.com/articles/2025-12-19-9) — 机器之心 · 2025-12-19 15:02
- [拉伸没有用？也许只是你拉伸不到位](https://sspai.com/post/104692) — 少数派 · 2025-12-19 15:00
- [大模型「越想越错」？人大&腾讯团队用信息论揭示：什么时候该想、什么时候别想](https://www.jiqizhixin.com/articles/2025-12-19-8) — 机器之心 · 2025-12-19 14:55
- [可打电话可对讲！全球首款奥特曼 AI 互动对话器来了](https://www.qbitai.com/2025/12/362076.html) — 量子位 · 2025-12-19 14:40
- [云宇星空大模型正式发布，上海市规划资源局与商汤大装置联合打造](https://www.qbitai.com/2025/12/362081.html) — 量子位 · 2025-12-19 14:34
- [谷歌、英伟达、OpenAI在列，美国能源部宣布与24家机构达成协议，共同推进「创世纪计划」](https://www.jiqizhixin.com/articles/2025-12-19-7) — 机器之心 · 2025-12-19 14:03
- [树莓派家庭服务器（下）：自动化追剧的配置](https://sspai.com/post/104711) — 少数派 · 2025-12-19 11:30


## 论文 · arXiv

- [Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification](https://arxiv.org/abs/2512.16921v1) — arXiv · LLM core · 2025-12-19 02:59
- [Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning](https://arxiv.org/abs/2512.16917v1) — arXiv · LLM core · 2025-12-19 02:59
- [Constructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates](https://arxiv.org/abs/2512.16914v1) — arXiv · LLM core · 2025-12-19 02:59
- [Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward](https://arxiv.org/abs/2512.16912v1) — arXiv · LLM core · 2025-12-19 02:59
- [Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos](https://arxiv.org/abs/2512.16907v1) — arXiv · MoE & Reasoning · 2025-12-19 02:59
- [How Good is Post-Hoc Watermarking With Language Model Rephrasing?](https://arxiv.org/abs/2512.16904v1) — arXiv · LLM core · 2025-12-19 02:57
- [Impacts of Racial Bias in Historical Training Data for News AI](https://arxiv.org/abs/2512.16901v1) — arXiv · LLM core · 2025-12-19 02:56
- [Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image](https://arxiv.org/abs/2512.16899v1) — arXiv · LLM core · 2025-12-19 02:56
- [In-Context Algebra](https://arxiv.org/abs/2512.16902v1) — arXiv · MoE & Reasoning · 2025-12-19 02:56
- [LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation](https://arxiv.org/abs/2512.16891v1) — arXiv · LLM core · 2025-12-19 02:52
- [AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2512.16883v1) — arXiv · LLM core · 2025-12-19 02:50
- [Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models](https://arxiv.org/abs/2512.16866v1) — arXiv · MoE & Reasoning · 2025-12-19 02:37
- [TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge](https://arxiv.org/abs/2512.16855v1) — arXiv · LLM core · 2025-12-19 02:27
- [Tiny Recursive Control: Iterative Reasoning for Efficient Optimal Control](https://arxiv.org/abs/2512.16824v1) — arXiv · MoE & Reasoning · 2025-12-19 02:05
- [From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs](https://arxiv.org/abs/2512.16795v1) — arXiv · MoE & Reasoning · 2025-12-19 01:27

---
数据源与分组在 config.yaml → sections / feeds 中可自定义。