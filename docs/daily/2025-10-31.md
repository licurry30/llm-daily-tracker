# LLM Daily Brief — 2025-10-31

- 生成时间：2025-10-31 20:46 (Asia/Shanghai)
- 抓取窗口：24h · 源数：14


## 官方 / 厂商 · 国外

- [Elevated errors for requests to Claude 4.1 Opus](https://status.claude.com/incidents/spwz6zkztbqh) — Anthropic · Status · 2025-10-31 19:40
- [Elevated errors on claude.ai](https://status.claude.com/incidents/s5f75jhwjs6g) — Anthropic · Status · 2025-10-31 18:56
- [Elevated errors for requests to Claude 4 Sonnet](https://status.claude.com/incidents/zdxjv49ydg0f) — Anthropic · Status · 2025-10-31 18:36
- [High error rate on AssistantsAPI 4o-mini](https://status.openai.com//incidents/01K8WH5NC214S6RQ0RVK04BPQ7) — OpenAI · Status · 2025-10-31 15:46


## 中文媒体

- [从 SD 到 Wan2.5-Preview，AI 视频 2025 质变启示录](http://www.geekpark.net/news/355850) — 极客公园 · 2025-10-31 18:24
- [本周看什么 | 最近值得一看的 10 部作品](https://sspai.com/post/103452) — 少数派 · 2025-10-31 18:22
- [从文字到语音交互，AI 的下一个爆发点可能是拥有自己的身体](http://www.geekpark.net/news/355848) — 极客公园 · 2025-10-31 18:05
- [让 AI 开口「像人」：最难的不是智能，是「嗓音」](http://www.geekpark.net/news/355846) — 极客公园 · 2025-10-31 18:00
- [大模型公司不搞浏览器搞Agent，实测找到原因了](https://www.qbitai.com/2025/10/347801.html) — 量子位 · 2025-10-31 16:57
- [Kimi开源新线性注意力架构，首次超越全注意力模型，推理速度暴涨6倍](https://www.qbitai.com/2025/10/347789.html) — 量子位 · 2025-10-31 16:49
- [旅行中一定会买什么：10 位少数派的旅途收藏与独家记忆](https://sspai.com/post/103404) — 少数派 · 2025-10-31 15:00
- [量子位2025年度榜单冲刺申报中！企业/产品/人物榜正在征集](https://www.qbitai.com/2025/10/347762.html) — 量子位 · 2025-10-31 13:57
- [OpenAI首个GPT-5找Bug智能体：全自动读代码找漏洞写修复](https://www.qbitai.com/2025/10/347735.html) — 量子位 · 2025-10-31 12:53
- [从文字到语音交互，AI 的下一个爆发点可能是拥有自己的身体](http://www.geekpark.net/news/355815) — 极客公园 · 2025-10-31 12:46
- [自动驾驶公司，正在标配飞书](https://www.qbitai.com/2025/10/347618.html) — 量子位 · 2025-10-31 12:31
- [从书房到工作室，只需咔嗒一声：Khadas Mind 2 模块化工作流体验](https://sspai.com/post/103412) — 少数派 · 2025-10-31 10:32


## 论文 · arXiv

- [Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with   the MME-CoF Benchmark](http://arxiv.org/abs/2510.26802v1) — arXiv · MoE & Reasoning · 2025-10-31 01:59
- [Gistify! Codebase-Level Understanding via Runtime Execution](http://arxiv.org/abs/2510.26790v1) — arXiv · LLM core · 2025-10-31 01:58
- [Defeating the Training-Inference Mismatch via FP16](http://arxiv.org/abs/2510.26788v1) — arXiv · LLM core · 2025-10-31 01:58
- [Remote Labor Index: Measuring AI Automation of Remote Work](http://arxiv.org/abs/2510.26787v1) — arXiv · MoE & Reasoning · 2025-10-31 01:58
- [LLMs Process Lists With General Filter Heads](http://arxiv.org/abs/2510.26784v1) — arXiv · LLM core · 2025-10-31 01:57
- [Clone Deterministic 3D Worlds with Geometrically-Regularized World   Models](http://arxiv.org/abs/2510.26782v1) — arXiv · MoE & Reasoning · 2025-10-31 01:56
- [STaMP: Sequence Transformation and Mixed Precision for Low-Precision   Activation Quantization](http://arxiv.org/abs/2510.26771v1) — arXiv · LLM core · 2025-10-31 01:53
- [AMO-Bench: Large Language Models Still Struggle in High School Math   Competitions](http://arxiv.org/abs/2510.26768v1) — arXiv · LLM core · 2025-10-31 01:52
- [Deep sequence models tend to memorize geometrically; it is unclear why](http://arxiv.org/abs/2510.26745v1) — arXiv · MoE & Reasoning · 2025-10-31 01:40
- [Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models](http://arxiv.org/abs/2510.26732v1) — arXiv · MoE & Reasoning · 2025-10-31 01:31
- [ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for   Efficient MoE Inference](http://arxiv.org/abs/2510.26730v1) — arXiv · LLM core · 2025-10-31 01:29
- [Unveiling Intrinsic Text Bias in Multimodal Large Language Models   through Attention Key-Space Analysis](http://arxiv.org/abs/2510.26721v1) — arXiv · LLM core · 2025-10-31 01:22
- [Value Drifts: Tracing Value Alignment During LLM Post-Training](http://arxiv.org/abs/2510.26707v1) — arXiv · LLM core · 2025-10-31 01:09
- [Delegated Authorization for Agents Constrained to Semantic Task-to-Scope   Matching](http://arxiv.org/abs/2510.26702v1) — arXiv · LLM core · 2025-10-31 01:07
- [The End of Manual Decoding: Towards Truly End-to-End Language Models](http://arxiv.org/abs/2510.26697v1) — arXiv · LLM core · 2025-10-31 01:01
- [LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits](http://arxiv.org/abs/2510.26690v1) — arXiv · MoE & Reasoning · 2025-10-31 00:59
- [Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models](http://arxiv.org/abs/2510.26683v1) — arXiv · MoE & Reasoning · 2025-10-31 00:53

---
数据源与分组在 config.yaml → sections / feeds 中可自定义。