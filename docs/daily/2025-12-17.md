# LLM Daily Brief — 2025-12-17

- 生成时间：2025-12-17 20:52 (Asia/Shanghai)
- 抓取窗口：24h · 源数：14


## 官方 / 厂商 · 国外

- [Elevated error rates on Sonnet 4](https://status.claude.com/incidents/4pw74ts6z930) — Anthropic · Status · 2025-12-17 19:20


## 中文媒体

- [腾讯大模型「变阵」：成立 AI Infra 部，姚顺雨出任首席 AI 科学家](http://www.geekpark.net/news/358147) — 极客公园 · 2025-12-17 19:59
- [摩尔线程算法一鸣惊人，图形学顶会夺银！已开源](https://www.qbitai.com/2025/12/361555.html) — 量子位 · 2025-12-17 17:50
- [时间、项目双维度，打造真正有用的工作笔记](https://sspai.com/post/103960) — 少数派 · 2025-12-17 17:41
- [官宣！姚顺雨出任腾讯首席AI科学家，带队大语言模型、AI Infra](https://www.jiqizhixin.com/articles/2025-12-17-17) — 机器之心 · 2025-12-17 17:28
- [分割一切、3D重建一切还不够，Meta开源SAM Audio分割一切声音](https://www.jiqizhixin.com/articles/2025-12-17-16) — 机器之心 · 2025-12-17 17:24
- [具透：微软改名部上线，Homebrew 弃用实用选项引起争议](https://sspai.com/prime/story/inside-release-notes-251217) — 少数派 · 2025-12-17 16:27
- [ChatGPT文风，原产地肯尼亚](https://www.qbitai.com/2025/12/361471.html) — 量子位 · 2025-12-17 16:10
- [爱诗科技与阿里云达成全栈AI合作 AI视频全球化再启航](https://www.jiqizhixin.com/articles/2025-12-17-15) — 机器之心 · 2025-12-17 15:31
- [骏马贺新岁，挑一本日历迎接腾跃的 2026 年](https://sspai.com/post/104604) — 少数派 · 2025-12-17 15:25
- [人车家全生态持续破圈，小米宣布对开发者开放小米MiMo大模型、CarIoT硬件生态](https://www.qbitai.com/2025/12/361460.html) — 量子位 · 2025-12-17 15:19
- [硬刚Sora2，万相2.6轻松定制角色、控制分镜，普通人也能当导演](https://www.jiqizhixin.com/articles/2025-12-17-14) — 机器之心 · 2025-12-17 14:47
- [人车家全生态持续破圈，小米宣布对开发者开放小米MiMo大模型、CarIoT硬件生态](https://www.jiqizhixin.com/articles/2025-12-17-13) — 机器之心 · 2025-12-17 14:43
- [是个公司都在用AI Agent，但大家真的用明白了吗| MEET2026圆桌论坛](https://www.qbitai.com/2025/12/361436.html) — 量子位 · 2025-12-17 13:10
- [英伟达护城河又宽了！低调收购开源算力调度王牌工具，全球过半顶级超算在用，Thinking Machines也离不开它](https://www.qbitai.com/2025/12/361414.html) — 量子位 · 2025-12-17 12:21
- [工业设计里的「打孔」简史：写在「扭扭宝」上市之初](https://sspai.com/post/104627) — 少数派 · 2025-12-17 11:30


## 论文 · arXiv

- [TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs](https://arxiv.org/abs/2512.14698v1) — arXiv · LLM core · 2025-12-17 02:59
- [Universal Reasoning Model](https://arxiv.org/abs/2512.14693v1) — arXiv · MoE & Reasoning · 2025-12-17 02:58
- [MMGR: Multi-Modal Generative Reasoning](https://arxiv.org/abs/2512.14691v1) — arXiv · MoE & Reasoning · 2025-12-17 02:58
- [Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization](https://arxiv.org/abs/2512.14687v1) — arXiv · LLM core · 2025-12-17 02:54
- [Fast and Accurate Causal Parallel Decoding using Jacobi Forcing](https://arxiv.org/abs/2512.14681v1) — arXiv · LLM core · 2025-12-17 02:45
- [TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines](https://arxiv.org/abs/2512.14645v1) — arXiv · MoE & Reasoning · 2025-12-17 02:02
- [LLmFPCA-detect: LLM-powered Multivariate Functional PCA for Anomaly Detection in Sparse Longitudinal Texts](https://arxiv.org/abs/2512.14604v1) — arXiv · LLM core · 2025-12-17 01:14
- [Towards Nepali-language LLMs: Efficient GPT training with a Nepali BPE tokenizer](https://arxiv.org/abs/2512.14585v1) — arXiv · LLM core · 2025-12-17 00:53
- [Low-Resource, High-Impact: Building Corpora for Inclusive Language Technologies](https://arxiv.org/abs/2512.14576v1) — arXiv · MoE & Reasoning · 2025-12-17 00:44
- [Polypersona: Persona-Grounded LLM for Synthetic Survey Responses](https://arxiv.org/abs/2512.14562v1) — arXiv · LLM core · 2025-12-17 00:33
- [Agreement Between Large Language Models and Human Raters in Essay Scoring: A Research Synthesis](https://arxiv.org/abs/2512.14561v1) — arXiv · LLM core · 2025-12-17 00:33
- [Counterfactual Explanations for Time Series Should be Human-Centered and Temporally Coherent in Interventions](https://arxiv.org/abs/2512.14559v1) — arXiv · MoE & Reasoning · 2025-12-17 00:31
- [VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models](https://arxiv.org/abs/2512.14554v1) — arXiv · LLM core · 2025-12-17 00:28
- [VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse](https://arxiv.org/abs/2512.14531v1) — arXiv · LLM core · 2025-12-17 00:08
- [Dynamic Learning Rate Scheduling based on Loss Changes Leads to Faster Convergence](https://arxiv.org/abs/2512.14527v1) — arXiv · LLM core · 2025-12-17 00:03
- [RecGPT-V2 Technical Report](https://arxiv.org/abs/2512.14503v1) — arXiv · MoE & Reasoning · 2025-12-16 23:40
- [Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling](https://arxiv.org/abs/2512.14474v1) — arXiv · MoE & Reasoning · 2025-12-16 23:07
- [Context-Picker: Dynamic context selection using multi-stage reinforcement learning](https://arxiv.org/abs/2512.14465v1) — arXiv · MoE & Reasoning · 2025-12-16 22:52

---
数据源与分组在 config.yaml → sections / feeds 中可自定义。