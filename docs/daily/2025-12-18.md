# LLM Daily Brief — 2025-12-18

- 生成时间：2025-12-18 20:51 (Asia/Shanghai)
- 抓取窗口：24h · 源数：14


## 官方 / 厂商 · 国外

- [ChatGPT intermittently not loading successfully after SSO authentication](https://status.openai.com//incidents/01KCQ4D84M0KCBPNCENAV6B8GZ) — OpenAI · Status · 2025-12-18 17:02


## 中文媒体

- [不卖「工具」卖「生产力」，百融云创如何用「硅基员工」打破AI落地僵局？](https://www.jiqizhixin.com/articles/2025-12-18-8) — 机器之心 · 2025-12-18 18:42
- [与Physical Intelligence同日发声：深度机智亮出「情境数采」杀手锏，具身智能的通用性天花板要被捅破了？](https://www.jiqizhixin.com/articles/2025-12-18-7) — 机器之心 · 2025-12-18 18:36
- [SIGGRAPH Asia 2025 | 只用一部手机创建和渲染高质量3D数字人](https://www.jiqizhixin.com/articles/2025-12-18-6) — 机器之心 · 2025-12-18 18:33
- [行啊AI PC！现在都能隔空测血压、检测皮肤了](https://www.qbitai.com/2025/12/361905.html) — 量子位 · 2025-12-18 18:21
- [不儿，这谁还能看出是AI演的视频啊](https://www.qbitai.com/2025/12/361846.html) — 量子位 · 2025-12-18 17:56
- [告别抽卡！一手实测字节刚放出的视频模型Seedance 1.5 pro](https://www.jiqizhixin.com/articles/2025-12-18-5) — 机器之心 · 2025-12-18 17:30
- [新玩意 230｜少数派的编辑们最近买了啥？](https://sspai.com/post/104706) — 少数派 · 2025-12-18 17:30
- [北大发布 ManualVLA：首个长程「生成–理解–动作」一体化模型，实现从最终状态自主生成说明书并完成操纵](https://www.jiqizhixin.com/articles/2025-12-18-4) — 机器之心 · 2025-12-18 17:19
- [火山引擎的「火」，是怎么点起来的](http://www.geekpark.net/news/358192) — 极客公园 · 2025-12-18 16:48
- [IF 2026 最大「黑马」：钢铁侠故事背后的温度和创新](http://www.geekpark.net/news/358191) — 极客公园 · 2025-12-18 16:33
- [人人都能当电影导演？万相 2.6 杀疯了：角色扮演、分镜控制，硬刚 Sora2](http://www.geekpark.net/news/358189) — 极客公园 · 2025-12-18 16:29
- [编辑部圆桌 | 从 Windows 迁移至 Mac，他们为什么如此选择](https://sspai.com/post/104699) — 少数派 · 2025-12-18 15:34
- [腾讯调整大模型组织架构：姚顺雨加盟，向总裁刘炽平汇报](https://www.qbitai.com/2025/12/361841.html) — 量子位 · 2025-12-18 15:02
- [“特斯拉延期交付机器人是卡在灵巧手上，中国灵巧手遥遥领先”| 灵心巧手@MEET2026](https://www.qbitai.com/2025/12/361803.html) — 量子位 · 2025-12-18 14:40
- [具身智能的数据难题，终于有了可规模化的解法](https://www.qbitai.com/2025/12/361743.html) — 量子位 · 2025-12-18 14:20
- [「速度猎人」成就指南：探寻驾驶之外的形形色色](https://sspai.com/post/104686) — 少数派 · 2025-12-18 11:40


## 论文 · arXiv

- [Dynamic Rebatching for Efficient Early-Exit Inference with DREX](https://arxiv.org/abs/2512.15705v1) — arXiv · LLM core · 2025-12-18 02:55
- [FrontierCS: Evolving Challenges for Evolving Intelligence](https://arxiv.org/abs/2512.15699v1) — arXiv · MoE & Reasoning · 2025-12-18 02:52
- [BashArena: A Control Setting for Highly Privileged AI Agents](https://arxiv.org/abs/2512.15688v1) — arXiv · LLM core · 2025-12-18 02:45
- [Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2512.15687v1) — arXiv · LLM core · 2025-12-18 02:44
- [Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers](https://arxiv.org/abs/2512.15674v1) — arXiv · LLM core · 2025-12-18 02:26
- [Explaining the Reasoning of Large Language Models Using Attribution Graphs](https://arxiv.org/abs/2512.15663v1) — arXiv · LLM core · 2025-12-18 02:15
- [Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning](https://arxiv.org/abs/2512.15662v1) — arXiv · LLM core · 2025-12-18 02:15
- [VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?](https://arxiv.org/abs/2512.15649v1) — arXiv · LLM core · 2025-12-18 01:58
- [How Much is Too Much? Exploring LoRA Rank Trade-offs for Retaining Knowledge and Domain Robustness](https://arxiv.org/abs/2512.15634v1) — arXiv · LLM core · 2025-12-18 01:44
- [Evaluating Metrics for Safety with LLM-as-Judges](https://arxiv.org/abs/2512.15617v1) — arXiv · LLM core · 2025-12-18 01:24
- [Behavior Tokens Speak Louder: Disentangled Explainable Recommendation with Behavior Vocabulary](https://arxiv.org/abs/2512.15614v1) — arXiv · LLM core · 2025-12-18 01:24
- [Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction](https://arxiv.org/abs/2512.15605v1) — arXiv · MoE & Reasoning · 2025-12-18 01:14
- [Bolmo: Byteifying the Next Generation of Language Models](https://arxiv.org/abs/2512.15586v1) — arXiv · MoE & Reasoning · 2025-12-18 00:46
- [IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion](https://arxiv.org/abs/2512.15581v1) — arXiv · MoE & Reasoning · 2025-12-18 00:40
- [Evaluating Large Language Models in Scientific Discovery](https://arxiv.org/abs/2512.15567v1) — arXiv · MoE & Reasoning · 2025-12-18 00:20

---
数据源与分组在 config.yaml → sections / feeds 中可自定义。