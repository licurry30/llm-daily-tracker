# LLM Daily Brief — 2026-02-05

- 生成时间：2026-02-05 21:10 (Asia/Shanghai)
- 抓取窗口：24h · 源数：14


## 中文媒体

- [Claude一个插件吓哭华尔街，软件公司集体暴跌，2万亿元一日蒸发](https://www.qbitai.com/2026/02/376576.html) — 量子位 · 2026-02-05 20:04
- [新玩意 235｜少数派的编辑们最近买了啥？](https://sspai.com/post/106090) — 少数派 · 2026-02-05 17:15
- [段鹏飞获马斯克晋升！特斯拉自动驾驶工程总监，武汉理工校友](https://www.qbitai.com/2026/02/376522.html) — 量子位 · 2026-02-05 16:27
- [强化学习远不是最优，CMU刚刚提出最大似然强化学习](https://www.jiqizhixin.com/articles/2026-02-05-11) — 机器之心 · 2026-02-05 16:05
- [ICLR 2026 Workshop二轮征稿开启：聚焦终身智能体的学习、对齐、演化](https://www.jiqizhixin.com/articles/2026-02-05-10) — 机器之心 · 2026-02-05 15:59
- [首个大规模记忆湖发布，AI Infra跑步进入“记忆”时代](https://www.qbitai.com/2026/02/376502.html) — 量子位 · 2026-02-05 15:28
- [满分五分，你给 Apple 的 2025 年打几分？](https://sspai.com/post/106060) — 少数派 · 2026-02-05 14:56
- [谷歌北大联手学术版Banana爆火，论文图表100%精确生成](https://www.qbitai.com/2026/02/376451.html) — 量子位 · 2026-02-05 14:15
- [中国第一，全球第二，视频大模型领军者生数科技完成超 6 亿元A+轮融资](https://www.jiqizhixin.com/articles/2026-02-05-8) — 机器之心 · 2026-02-05 14:09
- [智能必须基于世界模型？我们和蚂蚁灵波团队聊了聊](https://www.jiqizhixin.com/articles/2026-02-05-7) — 机器之心 · 2026-02-05 13:10
- [谷歌做了个论文专用版nano banana！顶会级Figure直出](https://www.jiqizhixin.com/articles/2026-02-05-6) — 机器之心 · 2026-02-05 13:07
- [让城市成为一个生命体：交大系酷哇发布WAM 2.0世界模型，剑指RoboCity终局](https://www.qbitai.com/2026/02/376429.html) — 量子位 · 2026-02-05 12:21
- [从川渝之旅聊自驾：解锁更自在的旅行体验](https://sspai.com/post/105661) — 少数派 · 2026-02-05 11:36


## 英文媒体 / Newsletter

- [[AINews] ElevenLabs $500m Series D at $11B, Cerebras $1B Series H at $23B, Vibe Coding -> Agentic Engineering](https://www.latent.space/p/ainews-elevenlabs-500m-series-d-at) — Latent Space · 2026-02-05 16:26


## 研究机构 / 开源社区

- [Paza: Introducing automatic speech recognition benchmarks and models for low resource languages](https://www.microsoft.com/en-us/research/blog/paza-introducing-automatic-speech-recognition-benchmarks-and-models-for-low-resource-languages/) — Microsoft Research · 2026-02-05 13:07


## 论文 · arXiv

- [Reinforced Attention Learning](https://arxiv.org/abs/2602.04884v1) — arXiv · LLM core · 2026-02-05 02:59
- [Rethinking the Trust Region in LLM Reinforcement Learning](https://arxiv.org/abs/2602.04879v1) — arXiv · LLM core · 2026-02-05 02:59
- [Contrastive Continual Learning for Model Adaptability in Internet of Things](https://arxiv.org/abs/2602.04881v1) — arXiv · MoE & Reasoning · 2026-02-05 02:59
- [Multi-Head LatentMoE and Head Parallel: Communication-Efficient and Deterministic MoE Parallelism](https://arxiv.org/abs/2602.04870v1) — arXiv · LLM core · 2026-02-05 02:57
- [Subliminal Effects in Your Data: A General Mechanism via Log-Linearity](https://arxiv.org/abs/2602.04863v1) — arXiv · LLM core · 2026-02-05 02:50
- [CoT is Not the Chain of Truth: An Empirical Internal Analysis of Reasoning LLMs for Fake News Generation](https://arxiv.org/abs/2602.04856v1) — arXiv · LLM core · 2026-02-05 02:43
- [Decomposed Prompting Does Not Fix Knowledge Gaps, But Helps Models Say "I Don't Know"](https://arxiv.org/abs/2602.04853v1) — arXiv · LLM core · 2026-02-05 02:39
- [El Agente Quntur: A research collaborator agent for quantum chemistry](https://arxiv.org/abs/2602.04850v1) — arXiv · MoE & Reasoning · 2026-02-05 02:38
- [El Agente Estructural: An Artificially Intelligent Molecular Editor](https://arxiv.org/abs/2602.04849v1) — arXiv · MoE & Reasoning · 2026-02-05 02:38
- [Fluid Representations in Reasoning Models](https://arxiv.org/abs/2602.04843v1) — arXiv · MoE & Reasoning · 2026-02-05 02:34
- [Are AI Capabilities Increasing Exponentially? A Competing Hypothesis](https://arxiv.org/abs/2602.04836v1) — arXiv · MoE & Reasoning · 2026-02-05 02:28
- [Horizon-LM: A RAM-Centric Architecture for LLM Training](https://arxiv.org/abs/2602.04816v1) — arXiv · LLM core · 2026-02-05 02:04
- [Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents](https://arxiv.org/abs/2602.04813v1) — arXiv · LLM core · 2026-02-05 01:59
- [SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization](https://arxiv.org/abs/2602.04811v1) — arXiv · MoE & Reasoning · 2026-02-05 01:58
- [OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models](https://arxiv.org/abs/2602.04804v1) — arXiv · LLM core · 2026-02-05 01:51
- [Team, Then Trim: An Assembly-Line LLM Framework for High-Quality Tabular Data Generation](https://arxiv.org/abs/2602.04785v1) — arXiv · LLM core · 2026-02-05 01:34

---
数据源与分组在 config.yaml → sections / feeds 中可自定义。